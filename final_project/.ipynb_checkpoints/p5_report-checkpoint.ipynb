{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Identify Fraud from Enron Email\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1.\tSummarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The Enron dataset is a email dataset that contains emails associated with more than 100 senior management staff of Enron Corporation, which filed bankrupcy in December 2001.  The goal of this project is to predict persons of interest in the dataset based on their financial and email features.\n",
    "\n",
    "The financial features includes salary, deferral_payments, total_payments, loan_advances, bonus, restricted_stock_deferred, deferred_income, total_stock_value, expenses, exercised_stock_options, long_term_incentives, restricted_stock, director_fees, and other.\n",
    "\n",
    "The email features include to_messages, from_poi_to_this_person, from_messages, from_this_person_to_poi, and shared_receipt_with_poi.\n",
    "\n",
    "I found there is an outlier in the dataset when looking at the salary and bonus data.  This record includes the total salary and total bonus of all the staff in the dataset.  I removed this record named \"TOTAL\".\n",
    "\n",
    "There is another record that has missing values for every feature.  I deleted this record named \"LOCKHART EUGENE E\".\n",
    "\n",
    "There is a third record that is not for a person, but rather it is for \"THE TRAVEL AGENCY IN THE PARK\".  It was deleted.\n",
    "\n",
    "There is a total of 146 records in the dataset.  There are 18 person of interest (poi) and 128 none-poi.  After deleting the two outliers, there are 143 records left.\n",
    "\n",
    "Of all the features, only poi has no missing value.  All the others have missing values.  For example, salary has 49 missing values, and to_message has 57 missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2.\tWhat features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.\n",
    "The number of messages are intergers and it can vary a lot between persons.  Therefore, I creatd two new features to calculate the ratio of messages from and to pois.  On is the ratio of messages that were sent to poi to the total messages that were sent out.  The other is the ratio of messages that were received from poi to the total messages that were received.\n",
    "\n",
    "I used SelectKBest, multiple k values, and three algorithms (naive bayes, SVC, and decision tree) to evaluate the features and selected the top five.  When k is three, the naive bayes algorithm has the best result.  The selected features are exercised_stock_options, total_stock_value, and bonus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 3.\tWhat algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?\n",
    "I tried naive bayes, SVC, and decision tree algorithms.  The best one is naive bayes.  The accuracy, precision, and recall are 0.907, 0.6, and 0.6, respectively.  Both the SVC and decision tree algorithms have lower accuracy, precision, and recall scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 4.\tWhat does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).\n",
    "Parameter tunning is to look for the best parameter that can fit the train set.  If it is not done correctly, the fit will not be generalized.  Over tunning can over fit the training features and therefore makes the prediction model biased (results in high accuracy in the training set, but low accuracy in the test set.)\n",
    "\n",
    "The naive bayes algorithm does not have parameters to tune, except the priors = None option.  I added this option, but it did not change the results.  If I selected a diffrent algorithm, such as SVC, I will use the GridSearchCV in sklearn.grid_search to tune the parameters.  There are three parameters kernel, C, gamma.  C controls the smoothness of the decision boundary.  A lower C will result in a smoother curve.  Gamma controls how much influence a single training record has.  A lower gamma means the traing point has to be closer to its neighers to be affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 5.\tWhat is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?\n",
    "The validation process is used to determine if the selected algorithm is generalized.  I split the dataset in order to validate the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 6.\tGive at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance.\n",
    "I used accuracy, precision, and recall as the evaluation metrics.  The selected algorithm's accuracy, precision, and recall are 0.907, 0.6, and 0.6, respectively.\n",
    "\n",
    "Accuracy is the number of test set records that was predicted correctly by the model, devided by the total number of records in the test set.\n",
    "\n",
    "There are three concepts to define before we go further to understand precision and accuracy.\n",
    "True positives: The record in the test set is true, and it was predicted as true as well.\n",
    "False positives: The record in the test set is false, but it was predicted as false.\n",
    "False negatives: The record in the test set is true, but it was predicted as false.\n",
    "\n",
    "Precision is the ratio of true positives to the sum of true positives and false positives.\n",
    "Recall is the ratio of true positives to the sum of true positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### References\n",
    "\n",
    "1. https://en.wikipedia.org/wiki/Enron\n",
    "2. https://en.wikipedia.org/wiki/Enron_scandal\n",
    "3. http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "4. https://www.cs.cmu.edu/~./enron/\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
